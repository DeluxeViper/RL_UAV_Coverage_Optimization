{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLPrOm6QKCB-",
        "outputId": "5e1bf124-d606-477d-f05b-f25d50b12120"
      },
      "outputs": [],
      "source": [
        "!pip3 install gymnasium numpy gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DO-CYAGtKLZf"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "from sklearn.cluster import KMeans\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import time\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/zs/f8c7x0_j5tv_94s9q6dxq5nr0000gn/T/ipykernel_49745/1624940463.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  rng_state = torch.load('rng_state.pth')\n"
          ]
        }
      ],
      "source": [
        "rng_state = torch.load('rng_state.pth')\n",
        "torch.set_rng_state(rng_state)\n",
        "np.random.seed(42)\n",
        "rng_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eOW_lQ5JKMk4"
      },
      "outputs": [],
      "source": [
        "# Define the Q-Network using a simple feedforward neural network\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 128)  # Increased neurons for better GPU utilization\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Flatten the input tensor\n",
        "        x = F.leaky_relu(self.fc1(x)) # Optimized for GPU\n",
        "        x = F.leaky_relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nSVNoacNKNvH"
      },
      "outputs": [],
      "source": [
        "# Define the DQN agent\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=10000)\n",
        "        self.gamma = 0.99  # discount rate\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.0005\n",
        "        self.batch_size = 256\n",
        "        self.model = QNetwork(state_size, action_size).to(device)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        state = torch.FloatTensor(state.flatten()).unsqueeze(0).to(device)\n",
        "        act_values = self.model(state)\n",
        "        return torch.argmax(act_values[0]).item()\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "        minibatch = random.sample(self.memory, self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
        "\n",
        "        states = torch.FloatTensor(states).to(device)\n",
        "        next_states = torch.FloatTensor(next_states).to(device)\n",
        "        rewards = torch.FloatTensor(rewards).to(device)\n",
        "        dones = torch.FloatTensor(dones).to(device)\n",
        "\n",
        "        q_values = self.model(states)\n",
        "        next_q_values = self.model(next_states)\n",
        "        q_target = q_values.clone()\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            q_target[i, actions[i]] = rewards[i] + (self.gamma * torch.max(next_q_values[i]) * (1 - dones[i]))\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss = self.criterion(q_values, q_target)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Decay epsilon after each replay\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "        return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "9WI1gfm_KPNs"
      },
      "outputs": [],
      "source": [
        "class UAVClusterEnv(gym.Env):\n",
        "    def __init__(self, num_users=10, num_uavs=3, area_size=100, max_steps=200):\n",
        "        super(UAVClusterEnv, self).__init__()\n",
        "        self.num_users = num_users\n",
        "        self.num_uavs = num_uavs\n",
        "        self.area_size = area_size\n",
        "        self.max_steps = max_steps  # Maximum number of steps in an episode\n",
        "        \n",
        "        # State space: positions of users and UAVs\n",
        "        self.observation_space = spaces.Box(low=0, high=area_size, shape=(self.num_users + self.num_uavs, 2))\n",
        "        \n",
        "        # Action space: UAVs can move in 4 directions (up, down, left, right)\n",
        "        self.action_space = spaces.Discrete(4 * self.num_uavs)\n",
        "        \n",
        "        # Initialize environment state\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.users = np.random.rand(self.num_users, 2) * self.area_size\n",
        "        \n",
        "        # Use K-means to find clusters among the users\n",
        "        kmeans = KMeans(n_clusters=self.num_uavs, n_init=10)\n",
        "        kmeans.fit(self.users)\n",
        "        self.uavs = kmeans.cluster_centers_  # Initialize UAVs at the centroids of the clusters\n",
        "        \n",
        "        self.user_velocities = np.random.uniform(-1, 1, (self.num_users, 2))\n",
        "        # self.uav_velocities = np.random.uniform(-1, 1, (self.num_uavs, 2))  # Add velocities for UAVs\n",
        "        self.current_step = 0\n",
        "        return self._get_state()\n",
        "\n",
        "    def step(self, action):\n",
        "        self.current_step += 1\n",
        "        \n",
        "        # Update UAV positions based on the selected actions\n",
        "        for i in range(self.num_uavs):\n",
        "            direction = action % 4  # Direction for each UAV (0: up, 1: down, 2: left, 3: right)\n",
        "            action = action // 4\n",
        "            \n",
        "            if direction == 0:  # Up\n",
        "                self.uavs[i][1] += 1\n",
        "            elif direction == 1: # Down\n",
        "                self.uavs[i][1] -= 1\n",
        "            elif direction == 2: # Left\n",
        "                self.uavs[i][0] -= 1\n",
        "            elif direction == 3:  # Right\n",
        "                self.uavs[i][0] += 1\n",
        "            \n",
        "            # Boundary check to prevent UAVs from moving outside the map\n",
        "            self.uavs[i][0] = np.clip(self.uavs[i][0], 0, self.area_size)  # Ensure x is within bounds\n",
        "            self.uavs[i][1] = np.clip(self.uavs[i][1], 0, self.area_size)  # Ensure y is within bounds\n",
        "            \n",
        "        # Update user positions\n",
        "        for i in range(self.num_users):\n",
        "            self.users[i] += self.user_velocities[i]\n",
        "            \n",
        "            # Boundary check to prevent users from moving outside the map\n",
        "            if self.users[i][0] < 0:\n",
        "                self.users[i][0] = 0\n",
        "                self.user_velocities[i][0] *= -1  # Reverse direction\n",
        "                self.user_velocities[i][1] = np.random.uniform(-1, 1)  # Randomize the y direction\n",
        "            elif self.users[i][0] > self.area_size:\n",
        "                self.users[i][0] = self.area_size\n",
        "                self.user_velocities[i][0] *= -1  # Reverse direction\n",
        "                self.user_velocities[i][1] = np.random.uniform(-1, 1)  # Randomize the y direction\n",
        "                \n",
        "            if self.users[i][1] < 0:\n",
        "                self.users[i][1] = 0\n",
        "                self.user_velocities[i][1] *= -1  # Reverse direction\n",
        "                self.user_velocities[i][0] = np.random.uniform(-1, 1)  # Randomize the x direction\n",
        "            elif self.users[i][1] > self.area_size:\n",
        "                self.users[i][1] = self.area_size\n",
        "                self.user_velocities[i][1] *= -1  # Reverse direction\n",
        "                self.user_velocities[i][0] = np.random.uniform(-1, 1)  # Randomize the x direction\n",
        "            \n",
        "            # Ensure the velocity isn't too small to cause the user to stop moving\n",
        "            if np.linalg.norm(self.user_velocities[i]) < 0.1:\n",
        "                self.user_velocities[i] += np.random.uniform(-0.5, 0.5, 2)  # Add a small random vector\n",
        "        \n",
        "        # Calculate the reward\n",
        "        reward = self._calculate_reward()\n",
        "        \n",
        "        # Check if the episode is done\n",
        "        done = self._is_done()\n",
        "\n",
        "        return self._get_state(), reward, done, {}\n",
        "\n",
        "    def _is_done(self):\n",
        "        # Terminal condition: episode ends after a certain number of steps\n",
        "        return self.current_step >= self.max_steps\n",
        "\n",
        "    def _get_state(self):\n",
        "        return np.concatenate([self.users, self.uavs])\n",
        "\n",
        "    def _calculate_reward(self):\n",
        "        # Reward is based on the distance between UAVs and users\n",
        "        total_reward = 0\n",
        "        for uav in self.uavs:\n",
        "            distances = np.linalg.norm(self.users - uav, axis=1)\n",
        "            coverage = np.sum(np.exp(-distances))  # Exponential decay with distance\n",
        "            total_reward += coverage\n",
        "\n",
        "            # Penalize UAV for being far from users\n",
        "            total_reward -= np.sum(distances) * 0.001  # Adjust weight as needed\n",
        "\n",
        "            # Penalize UAV for being close to the boundaries\n",
        "            boundary_penalty = 0.1 * (uav[0] / self.area_size + uav[1] / self.area_size)  # Closer to boundary, higher penalty\n",
        "            boundary_penalty += 0.1 * ((self.area_size - uav[0]) / self.area_size + (self.area_size - uav[1]) / self.area_size)\n",
        "            total_reward -= boundary_penalty\n",
        "        return total_reward\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        # Optional: Visualize the environment using a plotting library\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "QBLv_6ncKQmj"
      },
      "outputs": [],
      "source": [
        "def train_dqn_agent(env, agent, n_episodes=500, max_steps_per_episode=100, stop_loss_threshold=0.01, patience=10):\n",
        "    save_path = f\"dqn_weights_users{env.num_users}_uavs{env.num_uavs}_area{env.area_size}.pth\"\n",
        "    losses = []\n",
        "    rewards = []\n",
        "    stop_loss_counter = 0\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        episode_losses = []\n",
        "        start_time = time.time()\n",
        "\n",
        "        for step in range(max_steps_per_episode):\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.remember(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            \n",
        "            loss = agent.replay()\n",
        "            if loss is not None:\n",
        "                episode_losses.append(loss)\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        avg_loss = np.mean(episode_losses) if episode_losses else 0\n",
        "        epsilon = agent.epsilon\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        end_time = time.time()\n",
        "        episode_duration = end_time - start_time\n",
        "        avg_reward = np.mean(rewards[-100:])\n",
        "\n",
        "        print(f\"Episode {episode + 1}/{n_episodes} | Epsilon: {epsilon:.2f} | \"\n",
        "              f\"Avg Reward: {avg_reward:.2f} | Avg Loss: {avg_loss:.4f} | \"\n",
        "              f\"Time: {episode_duration:.2f}s\")\n",
        "\n",
        "        # Check for stop loss condition\n",
        "        if avg_loss < stop_loss_threshold:\n",
        "            stop_loss_counter += 1\n",
        "            print(f\"stop loss counter: {stop_loss_counter}\")\n",
        "            if stop_loss_counter >= patience:\n",
        "                print(f\"Training stopped early at episode {episode + 1} due to consistently low loss.\")\n",
        "                break\n",
        "        else:\n",
        "            stop_loss_counter = 0  # Reset counter if loss increases\n",
        "\n",
        "    # Save the model weights after training\n",
        "    torch.save(agent.model.state_dict(), save_path)\n",
        "    print(f\"Model weights saved to {save_path}\")\n",
        "\n",
        "    # Plotting the losses\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(losses, label=\"Loss\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Loss per Episode\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return rewards, losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGSy1cHmKSJg",
        "outputId": "d694b604-6677-42f5-e261-63f76ddd7de3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "QNetwork(\n",
              "  (fc1): Linear(in_features=26, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=12, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create the environment and DQN agent\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "env = UAVClusterEnv(num_users=10, num_uavs=3, area_size=15)\n",
        "state_size = np.prod(env.observation_space.shape)\n",
        "action_size = env.action_space.n\n",
        "\n",
        "# IF using non-existing model, use this code:\n",
        "agent = DQNAgent(state_size, action_size)\n",
        "agent.model.to(device)\n",
        "\n",
        "# IF using existing model, uncomment this code:\n",
        "\n",
        "# Initialize the model (make sure it matches the structure of the saved model)\n",
        "# agent = DQNAgent(state_size, action_size)\n",
        "\n",
        "# # Load the saved weights\n",
        "# agent.model.load_state_dict(torch.load(\"dqn_weights.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gPby80xKTaX",
        "outputId": "37322a50-6c54-4025-e918-809ba213b5c5"
      },
      "outputs": [],
      "source": [
        "# Train the DQN agent\n",
        "rewards, losses = train_dqn_agent(env, agent, n_episodes=100, max_steps_per_episode=100, stop_loss_threshold=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "dXqZi64UKi7W"
      },
      "outputs": [],
      "source": [
        "def test_dqn_agent(env, agent, n_episodes=5, max_steps_per_episode=100):\n",
        "    \"\"\"\n",
        "    Test the DQN agent in the environment for a given number of episodes with visualization.\n",
        "    \n",
        "    Args:\n",
        "        env: The environment to test in.\n",
        "        agent: The DQN agent.\n",
        "        n_episodes: Number of episodes to run.\n",
        "        max_steps_per_episode: Maximum number of steps per episode.\n",
        "    \n",
        "    Returns:\n",
        "        total_rewards: List of total rewards obtained in each episode.\n",
        "        metrics: Dictionary containing various evaluation metrics.\n",
        "    \"\"\"\n",
        "    total_rewards = []\n",
        "    avg_user_distances = []\n",
        "    coverage_overlaps = []\n",
        "    coverage_uniformities = []\n",
        "    normalized_rewards = []\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "\n",
        "        for step in range(max_steps_per_episode):\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "\n",
        "            # Clear the previous plot\n",
        "            clear_output(wait=True)\n",
        "\n",
        "            # Plot the environment\n",
        "            plt.figure(figsize=(8, 8))\n",
        "            plt.xlim(0, env.area_size)\n",
        "            plt.ylim(0, env.area_size)\n",
        "            plt.scatter(env.users[:, 0], env.users[:, 1], color='blue', label='Users')\n",
        "            plt.scatter(env.uavs[:, 0], env.uavs[:, 1], color='red', label='UAVs')\n",
        "\n",
        "            # Calculate and plot distances/rewards\n",
        "            total_coverage = np.zeros(env.users.shape[0])\n",
        "            for i, uav in enumerate(env.uavs):\n",
        "                distances = np.linalg.norm(env.users - uav, axis=1)\n",
        "                coverage = np.exp(-distances)  # Exponential decay with distance\n",
        "                total_coverage += coverage\n",
        "\n",
        "                for j, user in enumerate(env.users):\n",
        "                    plt.plot([uav[0], user[0]], [uav[1], user[1]], 'k-', lw=0.5)\n",
        "                    plt.text((uav[0] + user[0]) / 2, (uav[1] + user[1]) / 2, f'{coverage[j]:.2f}', fontsize=8, color='green')\n",
        "\n",
        "            # Plot coverage as a color intensity for each user\n",
        "            plt.scatter(env.users[:, 0], env.users[:, 1], c=total_coverage, cmap='coolwarm', s=200, alpha=0.6, edgecolor='black')\n",
        "            plt.colorbar(label='Coverage Intensity')\n",
        "\n",
        "            plt.title(f'Episode {episode + 1}, Step {step + 1}')\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Calculate metrics after the episode\n",
        "        avg_distance = np.mean([np.min(np.linalg.norm(env.users - uav, axis=1)) for uav in env.uavs])\n",
        "        coverage_overlap = sum(\n",
        "            [max(0, env.area_size / 3 - np.linalg.norm(env.uavs[i] - env.uavs[j]))\n",
        "             for i in range(len(env.uavs)) for j in range(i + 1, len(env.uavs))]\n",
        "        )\n",
        "        user_counts = np.zeros(len(env.uavs))\n",
        "        for user in env.users:\n",
        "            nearest_uav_idx = np.argmin(np.linalg.norm(env.uavs - user, axis=1))\n",
        "            user_counts[nearest_uav_idx] += 1\n",
        "        coverage_uniformity = np.var(user_counts)\n",
        "        normalized_reward = total_reward / (len(env.users) * len(env.uavs))\n",
        "\n",
        "        # Store the metrics\n",
        "        avg_user_distances.append(avg_distance)\n",
        "        coverage_overlaps.append(coverage_overlap)\n",
        "        coverage_uniformities.append(coverage_uniformity)\n",
        "        normalized_rewards.append(normalized_reward)\n",
        "\n",
        "        total_rewards.append(total_reward)\n",
        "        print(f\"Test Episode {episode + 1}/{n_episodes} | Total Reward: {total_reward:.2f} | Avg Distance: {avg_distance:.2f} | \"\n",
        "              f\"Overlap: {coverage_overlap:.2f} | Uniformity: {coverage_uniformity:.2f} | Normalized Reward: {normalized_reward:.4f}\")\n",
        "\n",
        "    avg_reward = np.mean(total_rewards)\n",
        "    avg_distance = np.mean(avg_user_distances)\n",
        "    avg_overlap = np.mean(coverage_overlaps)\n",
        "    avg_uniformity = np.mean(coverage_uniformities)\n",
        "    avg_normalized_reward = np.mean(normalized_rewards)\n",
        "\n",
        "    print(f\"\\nAverage Reward over {n_episodes} Test Episodes: {avg_reward:.2f}\")\n",
        "    print(f\"Average User Distance: {avg_distance:.2f}\")\n",
        "    print(f\"Average Coverage Overlap: {avg_overlap:.2f}\")\n",
        "    print(f\"Average Coverage Uniformity: {avg_uniformity:.2f}\")\n",
        "    print(f\"Average Normalized Reward: {avg_normalized_reward:.4f}\")\n",
        "\n",
        "    metrics = {\n",
        "        \"avg_reward\": avg_reward,\n",
        "        \"avg_distance\": avg_distance,\n",
        "        \"avg_overlap\": avg_overlap,\n",
        "        \"avg_uniformity\": avg_uniformity,\n",
        "        \"avg_normalized_reward\": avg_normalized_reward\n",
        "    }\n",
        "    \n",
        "    return total_rewards, metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 761
        },
        "id": "nHpjQwO1R70A",
        "outputId": "b178f54a-66d5-4823-c5d7-45cb32239bc9"
      },
      "outputs": [],
      "source": [
        "test_rewards = test_dqn_agent(env, agent, n_episodes=5, max_steps_per_episode=100)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
